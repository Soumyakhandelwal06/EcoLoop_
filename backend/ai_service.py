import os
import google.generativeai as genai
from dotenv import load_dotenv
import json
import time
from PIL import Image
import io

# 1. Load Environment Variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    print("CRITICAL WARNING: GOOGLE_API_KEY is missing from .env file.")

genai.configure(api_key=GOOGLE_API_KEY)

# --- SMART MODEL SELECTION ---

# --- SMART MODEL SELECTION ---

def get_model():
    """
    Returns a configured Gemini model, trying multiple versions if needed.
    """
    params = {
        "generation_config": {
            "temperature": 0.7,
            "max_output_tokens": 500,
        }
    }
    
    # Priority list based on available models
    models_to_try = [
        'gemini-2.0-flash',
        'gemini-2.0-flash-lite',
        'gemini-flash-latest', 
        'gemini-1.5-flash',
        'gemini-pro'
    ]

    for model_name in models_to_try:
        try:
            return genai.GenerativeModel(model_name, **params)
        except Exception:
            continue
            
    # Default fallback
    return genai.GenerativeModel('gemini-2.0-flash', **params)

# Initialize the model dynamically
model = get_model()


# --- SYSTEM PROMPTS ---

ECOLOOP_SYSTEM_PROMPT = """
You are 'EcoBot', the friendly AI assistant for the EcoLoop app.
EcoLoop is a gamified platform that rewards users for sustainable actions like recycling, saving water, and reducing carbon footprints.

YOUR RESPONSIBILITIES:
1. Motivate users to complete their daily eco-tasks.
2. Explain environmental concepts (Sustainability, 3Rs, Climate Change) simply.
3. Be encouraging, positive, and concise (keep answers under 3 sentences).
4. If a user asks about the app, explain that they can earn Coins and XP by uploading proof of their eco-actions.

Do not answer questions unrelated to the environment, nature, or the EcoLoop app.
"""

# --- FUNCTIONS ---

# Config for generation
params = {
    "generation_config": {
        "temperature": 0.7,
        "max_output_tokens": 500,
    }
}

models_to_try = [
    'gemini-2.0-flash', 
    'gemini-2.0-flash-lite',
    'gemini-1.5-flash',
    'gemini-1.5-flash-latest',
    'gemini-1.5-flash-001',
    'gemini-pro',
    'gemini-1.5-pro-latest'
]

async def get_chat_response(user_message: str):
    """
    Handles chat interactions for the EcoBot interface with robust failover.
    """
    full_prompt = f"{ECOLOOP_SYSTEM_PROMPT}\n\nUser: {user_message}\nEcoBot:"
    
    quota_error_hit = False

    for model_name in models_to_try:
        try:
            print(f"DEBUG: Trying Chat with model: {model_name}")
            model = genai.GenerativeModel(model_name, **params)
            response = await model.generate_content_async(full_prompt)
            return {"response": response.text}
        except Exception as e:
            error_str = str(e)
            print(f"âš ï¸ Model {model_name} failed: {e}")
            if "429" in error_str or "quota" in error_str.lower():
                quota_error_hit = True
            continue # Try next model

    # If all fail
    print("âŒ All AI models failed.")
    
    if quota_error_hit:
        return {
            "response": "I'm feeling a bit overwhelmed right now (Rate Limit Reached)! ðŸŒ¿ But remember: Every small action counts. Try asking me again in a minute!"
        }
        
    return {"response": "I'm having trouble connecting to the nature network right now. Try again later! ðŸŒ±"}

async def verify_task_content(file_path: str, mime_type: str, task_tag: str) -> dict:
    """
    Verifies if the uploaded content (Image or Video) matches the required task using Gemini.
    """
    if not GOOGLE_API_KEY:
        print("WARNING: GEMINI_API_KEY not found. Returning Mock Success.")
        return {
            "is_valid": True, 
            "message": "AI Verification Skipped (No API Key). Assuming success!", 
            "confidence": 1.0
        }

    try:
        # Use gemini-2.0-flash as it's the latest and great for multimodal
        # model is already initialized globally but let's ensure we use the right one for this call if needed
        # or just use the global 'model' variable.
        
        prompt = f"Analyze this media (could be image or video). Does it show {task_tag}? Answer ONLY with a JSON object: {{ 'valid': boolean, 'reason': string }}."

        if mime_type.startswith('video/'):
            # Video path
            print(f"DEBUG: Processing video with Gemini File API: {file_path}")
            
            # Upload the file
            video_file = genai.upload_file(path=file_path, mime_type=mime_type)
            
            # Wait for processing
            while video_file.state.name == "PROCESSING":
                print(".", end="", flush=True)
                time.sleep(1)
                video_file = genai.get_file(video_file.name)

            if video_file.state.name == "FAILED":
                raise Exception("Video processing failed at Google Gemini backend.")

            response = await model.generate_content_async([prompt, video_file])
            
            # Clean up: Files are stored for 48 hours, but we can delete manually if needed
            # For this prototype, we'll let them expire or add delete later.
        else:
            # Image path - using Path or PIL
            # The prompt code used PIL: Image.open(file_path)
            # But the user code provided: image = Image.open(file_path)
            # Since this function takes file_path, we can do that.
            image = Image.open(file_path)
            response = await model.generate_content_async([prompt, image])

        if not response or not hasattr(response, 'text'):
             return {
                "is_valid": False,
                "message": "AI could not process this media. Please try again with clear content.",
                "confidence": 0.0
            }

        text = response.text.replace('```json', '').replace('```', '').strip()
        
        try:
            result = json.loads(text)
            return {
                "is_valid": result.get("valid", False),
                "message": result.get("reason", "Analysis complete."),
                "confidence": 0.95 
            }
        except json.JSONDecodeError:
            is_valid = "true" in text.lower() or "yes" in text.lower()
            return {
                "is_valid": is_valid,
                "message": text,
                "confidence": 0.8
            }

    except Exception as e:
        import traceback
        traceback.print_exc()
        
        error_msg = str(e)
        if "429" in error_msg or "quota" in error_msg.lower():
             return {
                "is_valid": True,
                "message": "Quota exceeded, but verified for demo purposes! (Using Bypass)",
                "confidence": 1.0
            }

        return {
            "is_valid": False, 
            "message": f"AI Error: {error_msg}", 
            "confidence": 0.0
        }

# Keep the old function name as an alias or wrapper if legacy code calls it, 
# although main.py seems to have been updated to call verify_task_content.
# I will NOT include the old verify_task_submission to avoid confusion, assuming main.py is fully updated.

